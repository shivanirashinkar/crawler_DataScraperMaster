{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd7fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\91914\\firstoutput.txt.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel('C:\\\\Users\\\\91914\\\\OneDrive\\\\Desktop\\\\LAST SEM + FYP\\\\FYP\\\\china.xlsx', header=1)\n",
    "C:\\Users\\91914\\OneDrive\\Desktop\\LAST SEM + FYP\\FYP\\icsataackdata_fromattack_stix_data.xlsx\n",
    "\n",
    "# Select the columns you want to use and create a new DataFrame\n",
    "new_df = df[[ 'MITRE ATT&CK', 'Targets']]\n",
    "\n",
    "# Define the path and filename for the output file\n",
    "output_file = 'firstoutput.txt'\n",
    "\n",
    "# Open a new text file for writing\n",
    "with open(output_file, 'w') as f:\n",
    "    # Iterate over each row in the DataFrame and write the two columns as a sentence to the text file\n",
    "    for index, row in new_df.iterrows():\n",
    "        sentence = f\"{row['MITRE ATT&CK']} is related to {row['Targets']}.\"\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# Check if the file was created and print the full path to the file\n",
    "if os.path.isfile(output_file):\n",
    "    print(f'Output saved to {os.path.abspath(output_file)}.')\n",
    "else:\n",
    "    print('Error: output file was not created.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e08c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: C:\\Users\\91914\\output\\urlresults.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# specify the list of URLs to be scraped\n",
    "urls = [\n",
    "    \"https://attack.mitre.org/tactics/enterprise/\",\n",
    "    \"https://attack.mitre.org/techniques/mobile/\",\n",
    "    \"https://attack.mitre.org/tactics/ics/\",\n",
    "    \"https://www.dragos.com/\"\n",
    "]\n",
    "\n",
    "# specify the keyword(s) to search for\n",
    "keywords = [\"adversary\", \"attacks\", \"tactics\", \"vulnerabilities\",\"techniques\",\"cybersecurity\",\"ICS/OT\"]\n",
    "\n",
    "# specify the output directory and file name\n",
    "output_dir = \"./output\"\n",
    "output_file = \"urlresults.txt\"\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create or overwrite the output file\n",
    "with open(os.path.join(output_dir, output_file), \"w\") as f:\n",
    "    # loop over the list of URLs and scrape each one\n",
    "    for url in urls:\n",
    "        # send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # parse the HTML content of the website using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # find all the elements that contain the keyword(s)\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            for element in soup.find_all(string=lambda text: keyword in text.lower()):\n",
    "                if element.parent.name not in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                    results.append(element.strip())\n",
    "\n",
    "        # convert the list of results into sentences\n",
    "        sentences = []\n",
    "        for result in results:\n",
    "            sentences += re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', result)\n",
    "\n",
    "        # write the URL and the sentences to the output file\n",
    "        f.write(\"URL: \" + url + \"\\n\")\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + \"\\n\")\n",
    "\n",
    "# print the path to the output file\n",
    "print(\"Results saved to:\", os.path.abspath(os.path.join(output_dir, output_file)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fecd01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to: C:\\Users\\91914\\output\\otherblogsoutput.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# specify the list of URLs to be scraped\n",
    "\n",
    "\n",
    "# specify the output directory and file name\n",
    "output_dir = \"./output\"\n",
    "output_file = \"otherblogsoutput.txt\"\n",
    "\n",
    "# create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# create or overwrite the output file\n",
    "with open(os.path.join(output_dir, output_file), \"w\") as f:\n",
    "    # loop over the list of URLs and scrape each one\n",
    "    for url in urls:\n",
    "        # send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # parse the HTML content of the website using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # find all the elements that contain the keyword(s)\n",
    "        results = []\n",
    "        for keyword in keywords:\n",
    "            for element in soup.find_all(string=lambda text: keyword in text.lower()):\n",
    "                if element.parent.name not in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                    results.append(element.strip())\n",
    "\n",
    "        # convert the list of results into sentences\n",
    "        sentences = []\n",
    "        for result in results:\n",
    "            sentences += re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', result)\n",
    "\n",
    "        # write the URL and the sentences to the output file\n",
    "        f.write(\"URL: \" + url + \"\\n\")\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + \"\\n\")\n",
    "\n",
    "# print the path to the output file\n",
    "print(\"Results saved to:\", os.path.abspath(os.path.join(output_dir, output_file)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3018810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\91914\\icsdata.txt.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file into a DataFrame\\\n",
    "df = pd.read_excel('C:\\\\Users\\\\91914\\\\OneDrive\\\\Desktop\\\\LAST SEM + FYP\\\\FYP\\\\icsataackdata_fromattack_stix_data.xlsx', header=0)\n",
    "\n",
    "# Select the columns you want to use and create a new DataFrame\n",
    "new_df = df[[ 'name', 'description']]\n",
    "\n",
    "# Define the path and filename for the output file\n",
    "output_file = 'icsdata.txt'\n",
    "\n",
    "# Open a new text file for writing\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('Your text containing special characters')\n",
    "    # Iterate over each row in the DataFrame and write the two columns as a sentence to the text file\n",
    "    for index, row in new_df.iterrows():\n",
    "        sentence = f\"{row['name']} is related to {row['description']}.\"\n",
    "        f.write(sentence + '\\n')\n",
    "\n",
    "# Check if the file was created and print the full path to the file\n",
    "if os.path.isfile(output_file):\n",
    "    print(f'Output saved to {os.path.abspath(output_file)}.')\n",
    "else:\n",
    "    print('Error: output file was not created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24fb058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: C:\\Users\\91914\\Desktop\\outputreport.txt\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Define the keywords to search for\n",
    "keywords = ['Threat', 'Trojan', 'malware']\n",
    "\n",
    "# Define the output file path and name\n",
    "output_file = 'C:\\\\Users\\\\91914\\\\Desktop\\\\outputreport.txt'\n",
    "\n",
    "# Open the PDF file in binary mode\n",
    "with open('C:\\\\Users\\\\91914\\\\Downloads\\\\Threat_Intelligence_News_2023-03-20-1(march 13 -19).pdf', 'rb') as f:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(f)\n",
    "    # Get the total number of pages in the PDF file\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "\n",
    "    # Initialize a variable to store the extracted text\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Loop through each page in the PDF file\n",
    "    for i in range(num_pages):\n",
    "        # Get the text of the current page\n",
    "        page = pdf_reader.pages[i]\n",
    "        text = page.extract_text()\n",
    "\n",
    "        # Check if all the keywords are in the text\n",
    "        if all(keyword in text for keyword in keywords):\n",
    "            # Append the text that contains all the keywords to the extracted_text variable\n",
    "            extracted_text += text + \"\\n\"\n",
    "\n",
    "    # Save the extracted text to a text file\n",
    "    with open(output_file, 'w', encoding='utf-8') as output:\n",
    "        output.write(extracted_text)\n",
    "\n",
    "# Print the location of the saved text file\n",
    "print(\"Output saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df9d0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: scraped_data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.csoonline.com//\",\n",
    "    \"https://cybersecurity-insiders.tradepub.com//\",\n",
    "    \"https://threatpost.com//\"\n",
    "   \n",
    "   \n",
    "]\n",
    "\n",
    "# specify the keyword(s) to search for\n",
    "keywords = [\"Malware\", \"attacks\", \"THREATS\", \"vulnerabilities\",\"cybersecurity\",\"Webinars\",\"podcasts\"]\n",
    "\n",
    "\n",
    "# Create a variable to store the file path\n",
    "file_path = \"scraped_data.txt\"\n",
    "\n",
    "# Loop over the list of URLs and scrape each one\n",
    "for url in urls:\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the website using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the elements that contain the keyword(s)\n",
    "    results = []\n",
    "    for keyword in keywords:\n",
    "        for element in soup.find_all(string=lambda text: keyword in text.lower()):\n",
    "            if element.parent.name not in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                results.append(element.strip())\n",
    "\n",
    "    # Convert the list of results into sentences\n",
    "    sentences = []\n",
    "    for result in results:\n",
    "        sentences += re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', result)\n",
    "\n",
    "    # Print the URL and the sentences\n",
    "\n",
    "    # Save the data into a .txt file\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(\"URL: \" + url + \"\\n\")\n",
    "        for sentence in sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "# Print the path where the file is saved\n",
    "print(\"Data saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65b9baf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: darkredoutput.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "urls = [\n",
    "    \"https://www.scmagazine.com//\"\n",
    "   \n",
    "   \n",
    "   \n",
    "]\n",
    "\n",
    "# specify the keyword(s) to search for\n",
    "keywords = [\"Malware\", \"attacks\", \"THREATS\", \"vulnerabilities\",\"cybersecurity\",\"Google's .zip\", \" technology trends\", \"Analytics\", \n",
    "          \"  Attacks & Breaches\", \"Application Security\",\n",
    "            \"OAuth Flaw\", \"Cloud Security\",\" 'Volt Typhoon\", \"Internet of Things (IoT)\",\"IOT/ICS\", \"TOP STORIES\",\" SuperMailer\", \"Perimeter\", \"Risk\"]\n",
    "\n",
    "\n",
    "# Create a variable to store the file path\n",
    "file_path = \"darkredoutput.txt\"\n",
    "\n",
    "# Loop over the list of URLs and scrape each one\n",
    "for url in urls:\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the website using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the elements that contain the keyword(s)\n",
    "    results = []\n",
    "    for keyword in keywords:\n",
    "        for element in soup.find_all(string=lambda text: keyword in text.lower()):\n",
    "            if element.parent.name not in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                results.append(element.strip())\n",
    "\n",
    "    # Convert the list of results into sentences\n",
    "    sentences = []\n",
    "    for result in results:\n",
    "        sentences += re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', result)\n",
    "\n",
    "    # Print the URL and the sentences\n",
    "\n",
    "    # Save the data into a .txt file\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(\"URL: \" + url + \"\\n\")\n",
    "        for sentence in sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "# Print the path where the file is saved\n",
    "print(\"Data saved to:\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26330dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: DARK_LAST_SCHNEIER.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Specify the URLs and their respective keywords\n",
    "urls_keywords = {\n",
    "    \"https://www.darkreading.com//\": [\"adversary\", \"attacks\", \"tactics\", \"vulnerabilities\",\"cyber threats\",\" technology trends\"],\n",
    "    \"https://www.lastwatchdog.com//\": [\"ransomware\", \"threat\",\n",
    "                \"security\",\"Germany\",\"vehicles\",\"EMAIL\",\"Phishing\",\"RSA Conference 2023\",\"DigiCert\",\"RSAC\",\"cybercriminals\"],\n",
    "\"https://www.schneier.com//\": [\"ICS\", \"LLMs\", \"democracy\",\"AI\",\"Joe Biden\",\"loopholes\",\"political\"],\n",
    "    \"https://securityboulevard.com//security-bloggers-network//\":[\"Strategic Cybersecurity Cooperation Framework\",\"Arkose\",\n",
    "                                                 \"digital transformation\",\"CVE-2023-28771\",\"hybrid IT\",\"AI\",\"AppSec\",\"ChatGPT\",\"survey\"]\n",
    "}\n",
    "\n",
    "# Create a variable to store the file path\n",
    "file_path = \"DARK_LAST_SCHNEIER.txt\"\n",
    "\n",
    "# Loop over the URLs and scrape each one\n",
    "for url, keywords in urls_keywords.items():\n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content of the website using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all the elements that contain the keywords\n",
    "    results = []\n",
    "    for keyword in keywords:\n",
    "        for element in soup.find_all(string=lambda text: keyword in text.lower()):\n",
    "            if element.parent.name not in ['style', 'script', '[document]', 'head', 'title']:\n",
    "                results.append(element.strip())\n",
    "\n",
    "    # Convert the list of results into sentences\n",
    "    sentences = []\n",
    "    for result in results:\n",
    "        sentences += re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', result)\n",
    "\n",
    "    # Save the data into a .txt file\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(\"URL: \" + url + \"\\n\")\n",
    "        for sentence in sentences:\n",
    "            file.write(sentence + \"\\n\")\n",
    "\n",
    "# Print the path where the file is saved\n",
    "print(\"Data saved to:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1aaa5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the MITRE website page containing the TTP table\n",
    "url = 'https://attack.mitre.org/tactics/enterprise/'\n",
    "\n",
    "# Send a GET request to the website and retrieve the content\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the div containing the TTP table\n",
    "div = soup.find('div', {'class': 'container'})\n",
    "\n",
    "# Find the table within the div\n",
    "table = div.find('table')\n",
    "\n",
    "# Initialize empty lists for each column\n",
    "tactics = []\n",
    "techniques = []\n",
    "procedures = []\n",
    "\n",
    "# Extract data from each row of the table\n",
    "rows = table.find_all('tr')[1:]  # Skip the table header row\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    tactics.append(cells[0].text.strip())\n",
    "    techniques.append(cells[1].text.strip())\n",
    "    procedures.append(cells[2].text.strip())\n",
    "\n",
    "# Create a pandas DataFrame from the extracted data\n",
    "data = {'Tactic': tactics, 'Technique': techniques, 'Procedure': procedures}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('mitre_ttp_table.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d950b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table from https://attack.mitre.org/tactics/enterprise/ to mitre_table_0.xlsx\n",
      "Saved table from https://attack.mitre.org/tactics/mobile/ to mitre_table_1.xlsx\n",
      "Saved table from https://attack.mitre.org/tactics/ics/ to mitre_table_2.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/enterprise/ to mitre_table_3.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/mobile/ to mitre_table_4.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/ics/ to mitre_table_5.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/enterprise/ to mitre_table_6.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/mobile/ to mitre_table_7.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/ics/ to mitre_table_8.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of MITRE website page URLs containing TTP and Mitigations tables\n",
    "urls = [\n",
    "    'https://attack.mitre.org/tactics/enterprise/',\n",
    "    'https://attack.mitre.org/tactics/mobile/',\n",
    "    'https://attack.mitre.org/tactics/ics/',\n",
    "    'https://attack.mitre.org/techniques/enterprise/',\n",
    "    'https://attack.mitre.org/techniques/mobile/',\n",
    "    'https://attack.mitre.org/techniques/ics/',\n",
    "    'https://attack.mitre.org/mitigations/enterprise/',\n",
    "    'https://attack.mitre.org/mitigations/mobile/',\n",
    "    'https://attack.mitre.org/mitigations/ics/',\n",
    "    \n",
    "    # Add more URLs here\n",
    "]\n",
    "\n",
    "# Iterate over the URLs\n",
    "for i, url in enumerate(urls):\n",
    "    # Send a GET request to the website and retrieve the content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    # Locate the desired table by its position within the tables list\n",
    "    table = tables[0]  # Adjust the index based on the table position\n",
    "\n",
    "    # Initialize empty lists for each column\n",
    "    data = {'Tactic': [], 'Technique': [], 'Procedure': [], 'Mitigation': []}\n",
    "\n",
    "    # Extract data from each row of the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the table header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) == 3:\n",
    "            # TTP table\n",
    "            data['Tactic'].append(cells[0].text.strip())\n",
    "            data['Technique'].append(cells[1].text.strip())\n",
    "            data['Procedure'].append(cells[2].text.strip())\n",
    "            data['Mitigation'].append('')  # Fill in empty string for Mitigation column\n",
    "        elif len(cells) == 2:\n",
    "            # Mitigations table\n",
    "            data['Tactic'].append(cells[0].text.strip())\n",
    "            data['Technique'].append('')  # Fill in empty string for Technique column\n",
    "            data['Procedure'].append('')  # Fill in empty string for Procedure column\n",
    "            data['Mitigation'].append(cells[1].text.strip())\n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    filename = f'mitre_table_{i}.xlsx'\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "    print(f\"Saved table from {url} to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43668378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved table from https://attack.mitre.org/tactics/enterprise/ to mitre_tables\\ttpmitre_table_0.xlsx\n",
      "Saved table from https://attack.mitre.org/tactics/mobile/ to mitre_tables\\ttpmitre_table_1.xlsx\n",
      "Saved table from https://attack.mitre.org/tactics/ics/ to mitre_tables\\ttpmitre_table_2.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/enterprise/ to mitre_tables\\ttpmitre_table_3.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/mobile/ to mitre_tables\\ttpmitre_table_4.xlsx\n",
      "Saved table from https://attack.mitre.org/techniques/ics/ to mitre_tables\\ttpmitre_table_5.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/enterprise/ to mitre_tables\\ttpmitre_table_6.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/mobile/ to mitre_tables\\ttpmitre_table_7.xlsx\n",
      "Saved table from https://attack.mitre.org/mitigations/ics/ to mitre_tables\\ttpmitre_table_8.xlsx\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# List of MITRE website page URLs containing TTP and Mitigations tables\n",
    "urls = [\n",
    "    'https://attack.mitre.org/tactics/enterprise/',\n",
    "    'https://attack.mitre.org/tactics/mobile/',\n",
    "    'https://attack.mitre.org/tactics/ics/',\n",
    "    'https://attack.mitre.org/techniques/enterprise/',\n",
    "    'https://attack.mitre.org/techniques/mobile/',\n",
    "    'https://attack.mitre.org/techniques/ics/',\n",
    "    'https://attack.mitre.org/mitigations/enterprise/',\n",
    "    'https://attack.mitre.org/mitigations/mobile/',\n",
    "    'https://attack.mitre.org/mitigations/ics/',\n",
    "    \n",
    "    # Add more URLs here\n",
    "]\n",
    "\n",
    "\n",
    "# Directory to save the Excel files\n",
    "output_dir = 'mitre_tables'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the URLs\n",
    "for i, url in enumerate(urls):\n",
    "    # Send a GET request to the website and retrieve the content\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Create a BeautifulSoup object to parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all tables on the page\n",
    "    tables = soup.find_all('table')\n",
    "\n",
    "    # Locate the desired table by its position within the tables list\n",
    "    table = tables[0]  # Adjust the index based on the table position\n",
    "\n",
    "    # Initialize empty lists for each column\n",
    "    data = {'Tactic': [], 'Technique': [], 'Procedure': [], 'Mitigation': []}\n",
    "\n",
    "    # Extract data from each row of the table\n",
    "    rows = table.find_all('tr')[1:]  # Skip the table header row\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells) == 3:\n",
    "            # TTP table\n",
    "            data['Tactic'].append(cells[0].text.strip())\n",
    "            data['Technique'].append(cells[1].text.strip())\n",
    "            data['Procedure'].append(cells[2].text.strip())\n",
    "            data['Mitigation'].append('')  # Fill in empty string for Mitigation column\n",
    "        elif len(cells) == 2:\n",
    "            # Mitigations table\n",
    "            data['Tactic'].append(cells[0].text.strip())\n",
    "            data['Technique'].append('')  # Fill in empty string for Technique column\n",
    "            data['Procedure'].append(cells[1].text.strip())  # Rename Mitigation column to Procedure\n",
    "            data['Mitigation'].append('')\n",
    "\n",
    "    # Create a pandas DataFrame from the extracted data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Rename the column headers for the Mitigations table\n",
    "    if 'Mitigation' in df.columns:\n",
    "        df.rename(columns={'Mitigation': 'Procedure'}, inplace=True)\n",
    "\n",
    "    # Save the DataFrame to an Excel file\n",
    "    filename = f'ttpmitre_table_{i}.xlsx'\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_excel(filepath, index=False)\n",
    "\n",
    "    print(f\"Saved table from {url} to {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c8f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
